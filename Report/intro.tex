% What is style transfer?
\hspace{0.5cm} Style transfer (ST) is an image editing task, which given two examples, aims to synthesize a new image in a way that preserves the \textit{content} of one and the \textit{artistic style} of the other. Since many ST algorithms are trained to deal with predefined styles, an algorithm that works in a style agnostic manner is called Universal Style Transfer.
% What is in the UST paper?
In their paper "Universal Style Transfer via Feature Transforms" (NIPS 2017) \cite{bib11}, Li et al. show how the feature representations learned by high-performing CNNs (Encoder) followed by efficient feature Whitening-Coloring transforms (WCTs) and a compatible reconstruction (Decoder) can be used to perform the ST task. In their algorithm, which we refer to as UST throughout this paper, the two key building blocks are the encoder-decoder pairs and the WCT transform. The encoders are VGG based CNN feature extractors. Every such encoder has a matching decoder with symmetric architecture to perform image-reconstruction through feature-inversion. The style transfer process is conducted by transforming the content features at intermediate layers according to the statistics of the style features. This transformation takes place right after feature extraction (encoder) and before image reconstruction (decoder). Li et al. found that the classical signal whitening and coloring transforms are very efficient to this purpose.\\
% What are out results and innvoations?
In this project we implemented the UST algorithm in PyTorch. The goal for this implementation is to be a useful Style Transfer CLI tool. It is ready for use once downloaded from our GitHub: \url{https://github.com/eyalw711/universal_style_transfer}. Additionally, since it is designed to have lighter code volume than existing implementations, and offers more control, it should also appeal to researchers wanting to study various aspects of UST. Furthermore, in this project we trained decoder models for image reconstructions and experimented with sequentially and separately training each network block at a time. For this part we offer insight as to the time and resources it takes to train these models. Finally, as part of the project, we conducted two studies for methods to improve the UST algorithm, and both of which yielded results. In the first study, we looked at methods to synthetically increase the number channels in the feature representation of the input images, hoping to gain a stronger style transfer effect. This study yielded the innovative \textit{Boost} step, which we demonstrate to yield results with finer details and richer colors. The other study focused on new, efficient ways to transfer style from two style images at once. This study yielded three new techniques to perform this task, all of which are computationally more efficient than the one proposed in \cite{bib11} while demonstrated to have on-par results. \\

Our main contributions and results:
\begin{itemize}
	\item PyTorch Tool ready to be used by users to perform style transfer, and by researchers in order to study new models, methods or use-cases for UST.
	\item We present innovative approach to enhance the style transfer effect and demonstrate its effectiveness in producing visually pleasing results featuring more details and colors.
	\item We present new efficient techniques to perform ST from two style images at once. We show that these techniques produce visually pleasing results while using less computational resources thus saving runtime. 
\end{itemize}

%In this project, we implement the UST algorithm as a PyTorch Tool both ready to be used by users to perform style transfer, and to be used by researchers in order to study new models, methods or use-cases for UST.
%We introduce a novel methods which boosts style transfer by taking advantage of the existence of feature representations from state-of-the-art CNNs.  
%We also show new and efficient methods of combining different style images into the target image (content image) by using WCT algorithm efficiently.
%Our goal was to invent new and efficient ways of UST based on the work by Li et al. \cite{bib11}.
%Our method consists of a stylization step and a smoothing step. Both have a closed-form solution and can be computed efficiently. The stylization step is based on the (WCT) \cite{bib10}, which stylizes images via feature projections. The WCT was designed for artistic stylization.
%Our results show similar results as presented in \cite{bib10} while showing efficiency in computation.\\


% sentence pool
%The key challenge is how to extract effective representations of the style and then match it in the content image.
%Transferring the style from one image onto another can be considered a problem of texture transfer. In texture transfer the goal is to synthesize a texture from a source image while constraining the texture synthesis in order to preserve the semantic content of a target image. For texture synthesis there exist a large range of powerful non-parametric algorithms that can synthesise photo realistic natural textures by re-sampling the pixels of a given source texture \cite{bib1, bib2, bib3, bib4}. Most previous texture transfer algorithms rely on these non-parametric methods for texture synthesis while using different ways to preserve the structure of the target image. For instance, Efros and Freeman introduce a correspondence map that includes features of the target image such as image intensity to constrain the texture synthesis procedure \cite{bib3}. Lee et al. improve this algorithm by additionally informing the texture transfer with edge orientation information \cite{bib5}. Although these algorithms achieve remarkable results, they all suffer from the same fundamental limitation: they use only low-level image features of the target image to inform the texture transfer. Ideally, however, a style transfer algorithm should be able to extract the semantic image content (e.g. the objects and the general scenery) and then inform a texture transfer procedure to render the semantic content of the target image (content and style image) in the style of the style image. Therefore, a fundamental prerequisite is to find image representations that independently model variations in the semantic image content and the style in which it is presented.
%To generally separate content from style in natural images is still an extremely difficult problem.\newline
%However, the recent advance of Deep Convolutional Neural Networks (CNNs) \cite{bib6} has produced powerful computer vision systems that learn to extract high-level semantic information from natural images.  It was shown that CNNs trained with sufficient labeled data on specific tasks such as object recognition learn to extract high-level image content in generic feature representations that generalize across data sets \cite{bib7} and even to other visual information processing tasks, including texture recognition \cite{bib8} and artistic style classification \cite{bib15}.\newline
%The main issue is how to properly and effectively apply the extracted style characteristics (feature correlations) to content images in a style-agnostic manner.\newline

