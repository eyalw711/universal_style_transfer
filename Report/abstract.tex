\begin{abstract}
Style transfer is the technique of recomposing a content image with the artistic style of another. In this project, we implement an algorithm for \textit{Universal} Style Transfer, which means it is not limited to predefined styles. This is done through Feature Transforms of whitening and coloring. Using feature extractors and inverters, we are able to apply these transforms to deep features of the content image in order to match their statistics to those of the style image. This covariance matching essentially transfers the style while preserving the content, so the image reconstructed by the feature inverter is a style transferred image. Our implementation serves as a PyTorch tool ready to be used by users and researchers. Moreover, we offer an innovative approach to enhance the style transfer effect, and other approaches to transfer style from a pair of style images more efficiently than before.
\end{abstract}
%	Style transfer is the technique of recomposing images in the style of other images.
%Universal style transfer aims to transfer arbitrary visual styles to content images.
%Existing feed-forward based techniques would need to be trained on pre-defined styles and then fine tuned for new styles. Whereas, this paper presents new methods which are completely independent of the style during train phase making it a “learning-free” approach.
%In this paper, we present an encoder-decoder architecture where the encoder serves as feature-extractor and the decoder is trained for image reconstruction. 
%The Goal is to build a new Image which has the contents of the Content Image and style of the Style Image.
%Our results provide new insights handling arbitrary styles in an efficient-Learning-free methods.